{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of spacy_ner.ipynb",
      "provenance": [],
      "mount_file_id": "1L7ku04dSi2zjmjjnmp1pN4UFAjIvJoqK",
      "authorship_tag": "ABX9TyPu+6RjhjY4i801HVBE2M/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julianflowers/herbivores_ghg/blob/master/Copy_of_spacy_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook outlines how to use natural language processing (NLP) to extract locations and numbers from documents\n",
        "\n",
        "It uses Python modules (like R packages) which are designed for NLP and find relevant information in texts. This process is known as *named entity recognition* (NER) and relies on language models - very large datasets of words which have been pre-grouped into categories (*trained*), and breaking down texts into words (*tokens*) which are then matched with these datasets and assigned a category if one is found. This process is called *annotation*. "
      ],
      "metadata": {
        "id": "Nw-5rM8MS6De"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started"
      ],
      "metadata": {
        "id": "zNfp6AsAUyM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is install relevant packages and load them into Python. The gold standard NLP package is called `spacy` https://spacy.io/. We will also install a core language model which has been trained on the whole of wikipedia as well as other datasets like pubmed - this is called `en_core_web_lg` (**l**ar**g**e **en**glish core web-based model). "
      ],
      "metadata": {
        "id": "Nn0adO-5Uh2P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtLfL_JEQj0S"
      },
      "outputs": [],
      "source": [
        "! pip install spacy\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also install `spacypdfreader` - which unsurpisingly reads pdfs into the colab python environment"
      ],
      "metadata": {
        "id": "xyg_A53sV2mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eJCohYXXTQl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacypdfreader\n"
      ],
      "metadata": {
        "id": "0e_0TCraTE8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacypdfreader import pdf_reader"
      ],
      "metadata": {
        "id": "UwolNSHOTSO3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally we can upload some files to process..."
      ],
      "metadata": {
        "id": "UIccsAsqWK67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Ju_hQoj-cVy7",
        "outputId": "3e80e0f3-b530-412a-830c-cc7ca829bab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User uploaded file \"s13717-020-00230-z\" with length 1827044 bytes\n",
            "User uploaded file \"sustainability-12-02425-v2.pdf\" with length 2986118 bytes\n",
            "User uploaded file \"agronomy-11-01421-v2.pdf\" with length 685973 bytes\n",
            "User uploaded file \"Testing%20DayCent%20and%20DNDC%20model%20simulations%20of%20N2O%20fluxes%20and%20assessing%20the%20impacts%20of%20climate%20change%20on%20the%20gas%20flux%20and%20biomass%20production%20from%20a%20humid%20pasture.pdf\" with length 1938112 bytes\n",
            "User uploaded file \"s42452-020-03538-9.pdf\" with length 1365197 bytes\n",
            "User uploaded file \"9995c9d0bab3b678e4835d05490b4c993120.pdf\" with length 2305986 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named entity recognition\n",
        "\n",
        "We need to load the language model. The large model has 18 categories. Of most interest will probably be:\n",
        "\n",
        "\n",
        "*   CARDINAL - numbers\n",
        "*   GPE - geopolitical entities (e.g. countries)\n",
        "*    LOC - non-GPE locations\n",
        "*   DATE\n",
        "*  PERS\n",
        "*  ORG\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F7MqGC2bWYGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = spacy.load(\"en_core_web_lg\")\n"
      ],
      "metadata": {
        "id": "N1VQUpusQtQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ee3da5-13a1-44e2-b6c6-af0a8e3243d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_lg' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  message = f\"Error running command:\\n\\n{cmd_str}\\n\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in a pdf to a text object"
      ],
      "metadata": {
        "id": "Vlt3BmQqWg7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = pdf_reader(\"/content/sustainability-12-02425-v2.pdf\", ner )"
      ],
      "metadata": {
        "id": "-rzeWQpIReYj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "_I8QxyVyTrbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And run the code below to visualised how the NLP process has annotated the document"
      ],
      "metadata": {
        "id": "IKacPKEUWlsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(text,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "id": "1gJQq614WQaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or see a list of the named entities"
      ],
      "metadata": {
        "id": "9yTSmvVWWxEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner(text)\n",
        "for entity in text.ents:\n",
        "    print(entity.text,entity.label_)"
      ],
      "metadata": {
        "id": "hbAMO57-RzAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try another document"
      ],
      "metadata": {
        "id": "uORAXoloW1_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = pdf_reader(\"/content/s42452-020-03538-9.pdf\", ner)"
      ],
      "metadata": {
        "id": "7rcwIHIpZUw6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(text1,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "id": "CMTkSQZOZtKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can just extract the entities we need by creating a list of all the text annotated in the category of interest e.g. LOC"
      ],
      "metadata": {
        "id": "Clz5_-D9W62Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_loc=[]\n",
        "\n",
        "# Appending entities which have the label 'LOC' to the list\n",
        "for entity in text1.ents:\n",
        "  if entity.label_=='LOC':\n",
        "    list_of_loc.append(entity.text)\n",
        "\n",
        "list_of_gpe=[]\n",
        "\n",
        "# Appending entities which have the label 'LOC' to the list\n",
        "for entity in text1.ents:\n",
        "  if entity.label_=='GPE':\n",
        "    list_of_gpe.append(entity.text)"
      ],
      "metadata": {
        "id": "Ne_XEZMzVLwz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_loc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSEKW3gaal6b",
        "outputId": "9f7b02fc-1a5e-4da7-e0cc-a261c7f3d0f9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Africa',\n",
              " 'Northern Ethiopia',\n",
              " 'Tigray',\n",
              " 'Northern Ethiopia',\n",
              " 'Tigray',\n",
              " 'Tigray',\n",
              " 'Northern Ethiopia',\n",
              " 'Northern Ethiopia',\n",
              " 'Mai-Saba',\n",
              " 'Northern Ethiopia',\n",
              " 'North West',\n",
              " 'Map',\n",
              " 'Northern Ethiopia',\n",
              " 'Southern \\nTigray',\n",
              " 'Southern Tigray',\n",
              " 'South Western Ethiopia',\n",
              " 'Illubabor',\n",
              " 'South \\nWestern Ethiopia',\n",
              " 'Northern China',\n",
              " 'South Wello Zone',\n",
              " 'Central highlands',\n",
              " 'Butajira Area',\n",
              " 'the Middle Silluh Valley',\n",
              " 'Northern Ethiopia',\n",
              " 'Northern Ethiopia',\n",
              " 'Pampa',\n",
              " 'Nile',\n",
              " 'Northern \\nEthiopia',\n",
              " 'Southern Ethiopia',\n",
              " 'North-Western Ethiopia',\n",
              " 'Northern Ethiopia',\n",
              " 'J Arid Environ',\n",
              " 'North Western Zone',\n",
              " 'Africa',\n",
              " 'the Tengger Desert',\n",
              " 'Northern Ethiopia',\n",
              " 'Central New \\nSouth Wales',\n",
              " 'Ethiopian Rift',\n",
              " 'Northern Ethiopia',\n",
              " 'Inner Mongolia',\n",
              " 'Southern Ethiopia',\n",
              " 'Peninsular Malaysia',\n",
              " 'Southern']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_gpe"
      ],
      "metadata": {
        "id": "R-OmgxdVa34L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_numbers=[]\n",
        "\n",
        "\n",
        "# Appending entities which have the label 'LOC' to the list\n",
        "for entity in text1.ents:\n",
        "  if entity.label_=='CARDINAL':\n",
        "    list_of_numbers.append(entity.text)\n",
        "\n",
        "list_of_numbers"
      ],
      "metadata": {
        "id": "e-_jC2m-Lp6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and then we can write these lists to text files which we can open in Excel (or R) for further analysis or filtering."
      ],
      "metadata": {
        "id": "H4nPyemyX93v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = list_of_numbers\n",
        "with open(\"numbers.txt\", mode=\"w\") as file:\n",
        "    file.write(\"\\n\".join(names))\n",
        "\n",
        "locs = list_of_loc\n",
        "with open(\"locs.txt\", mode='w') as file:\n",
        "     file.write(\"\\n\".join(locs))"
      ],
      "metadata": {
        "id": "EhvEakPfOylp"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}