---
title: "Extracting lat longs from published articles"
format: html
editor: visual
execute:
    cache: false
    message: false
    warning: false
    echo: true
toc: true
toc-location: left
code-fold: true
code-tools: true
---

## Introduction

Ecological field studies usually report site location or range.

This note explains how to programmatically extract lat-longs and elevations from pdf publications for further processing (e.g. mapping).

One of the challenges is that there is not consistent reporting pattern so this is based on analyses of 45 documents which form part of the herbivory climate change review.

## Method

I outline 3 steps:

1.  Reading pdfs into R
2.  Designing the code to identify lat longs on the extracted text - this uses a process called regular expressions
3.  Onward processing of the extracted lat longs into decimal coordinated (if they are not already in decimal format).

### Reading pdfs into R

There are 2 R packages widely used for reading text and pdf files into R - `readtext` and `pdftools`.

In this note I will use `readtext`

I've already created a directory to store the pdfs I want to extract the information from.

```{r}
#| label: load libraries

library(pacman)  ## package manager
p_load(tidyverse, readtext, sf, mapview, leaflet, leafpop, quanteda, tidytext) ## installs and loads packages if not already installed or loaded


```

It's really easy to use `readtext`

```{r}
#| label: readtext

p <- here::here("my_corpus")   ## point R at the pdf directory 

f <- list.files(p, "pdf$", full.names = T)         ## get a list of files

df <- map_dfr(f, readtext)                        ## iterate through file list, read in the text and create a data frame

df                                       ## look at first 6 rows of dataframe
```

There are 2 columns - the file name of document, and the text effectively stored in a cell in the dataframe.

We can now generate a piece of text (regular expression) to try and match lat-longs in the documents.

```{r}
decimal_pattern <- "\\d{1,2}\\.\\d{1,2}◦.*[NSEW]|[NSEW]\\d{1,2}\\.\\d{1,2}◦|\\d{1,2}\\.\\d{1,2}°.*[NSEW]|[NSEW]\\d{1,2}\\.\\d{1,2}°"

colon_pattern <- "[NSEWnsew]?\\d{1,2}:\\d{1,2}:\\d{1,2}?"

normal_pattern <- "\\d{1,2}.*◦\\D?\\d{1,2}.*[NSEW]|\\d{1,2}.*°\\D?\\d{1,2}.*[NSEW]"

esting_nthing_pattern <- "\\d{5,6}\\D?[NSEW]"

degrees <- "\\d{1,2}\\D?(?=◦)|\\d{1,2}\\D?(?=°)"

decimal_coords <- "\\d{1,2}\\.\\d{1,2}.?◦.?[nsew]|\\d{1,2}\\.\\d{1,2}.?°.?[nsew]"
minutes <- "\\d{1,2}\\D?(?=ʹ)|\\d{1,2}\\D?(?=')"

degrees_minutes <-  "\\d{1,2}.?°.*\\d{1,2}.*'.*[NnSsEeWw]|\\d{1,2}\\D.?◦.*\\d{1,2}.*'.*[NnSsEeWw]|\\d{1,2}.?°.*\\d{1,2}.*ʹ.*[NnSsEeWw]|\\d{1,2}\\D.?◦.*\\d{1,2}.*ʹ.*[NnSsEeWw]"

str_match("20°45'N", degrees_minutes)

```

-   \\\\d{1,2}◦\\\\s\*\\\\d{1,2}.\*\[NSEW\] matches 20◦45'N. Note that in some pdfs what is printed as ° actually turns out to be ◦ when the pdf text is extracted

-   \\\\d{1,2}°\\\\s\*\\\\d{1,2}.\*\[NSEW\] matches 20°45'N.

-   \\\\d{1,2}\\\\.\\\\d{1,2}◦.\*\[NSEW\] matches 20.45◦W

-   \\\\d{1,2}\\\\.\\\\d{1,2}°\*\[NSEW\] matches 20.45°E

-   \\\\d{1,2}:\\\\d{1,2}:\\\\d{1,2}.\*\[NSEW\] matches 20:45:11 W

Sometimes to compass point is at the beginning rather than the end of the expression.

Regular expressions are an essential bit of coding needed to extract information from text but looks like gibberish.

### Elevations

For elevations a quick review of the papers suggest that there are a number of text patterns

-   `elevation`

-   `above see level`

-   `a.s.l`

Lets use these to try and extract elevations from text.

Potential text patterns are.

-   "\\\\d{1,}.\*elevation\|above sea level\|a.s.l" f*ind a number with at least 1 digit followed by the words elevation OR a.s.l OR above sea level*

-   "elevation\|above sea level\|a.s.l.\*\\\\d{1,}" *find the words elevation OR a.s.l OR above sea level followed by a number with 1 or more digits*

```{r}


 corpus <- corpus(df, text_field = "text")
 kw <- kwic(corpus, "elevation|a\\.s\\.l", valuetype = "regex")
 
 kw
```

```{r}
#| label: elevation 

df$text |>
  str_extract("\\d{1,}.*m.*(elevation|a\\.s\\.l.|above sea Level)|(elevation|a\\.s\\.l.|above sea Level).*\\d{1,}.*m")



```

```{r}

df_para <- df |>
  unnest_tokens(para, text, token = "paragraphs") 

df_para$para

df_2 <- df_para |>
  mutate(para_id = row_number(), 
         clean_para = str_remove_all(para, "°[Cc]"), 
         degrees = str_extract_all(clean_para, degrees_minutes), 
         decimal_coords = str_extract_all(clean_para, decimal_coords), 
         colon_coords = str_extract_all(clean_para, colon_pattern), 
         esting_coords = str_extract_all(clean_para, esting_nthing_pattern))


df_2 |>
  hoist("degrees") |>
  hoist("decimal_coords") |>
  hoist("colon_coords") |>
  unnest("esting_coords") |>
  View()

```

```{r}
df_2 <- df_2 |>
         mutate(elevation = str_extract_all(clean_para, "\\d{1,}.*m.*(elevation|a\\.s\\.l.|above sea Level)|(elevation|a\\.s\\.l.|above sea Level).*\\d{1,}.*m")) 


```

Lets apply these patterns to our texts

### Finding lat-longs

```{r}

# pattern_lat <- "\\d{1,2}◦\\s*\\d{1,2}.*[NS]|\\d{1,2}\\.\\d{1,2}◦.*[NS]|\\d{1,2}\\.\\d{1,2}°*[NS]|[NS]?\\d{1,2}:\\d{1,2}:\\d{1,2}?"
# 
# pattern_long <- "\\d{1,2}◦\\s*\\d{1,2}.*[EW]|\\d{1,2}\\.\\d{1,2}◦.*[EW]|\\d{1,2}\\.\\d{1,2}°*[EW]|[EW]?\\d{1,2}:\\d{1,2}:\\d{1,2}?"

decimal_pattern <- "\\d{1,2}\\.\\d{1,2}◦.*[NSEW]|[NSEW]\\d{1,2}\\.\\d{1,2}◦|\\d{1,2}\\.\\d{1,2}°.*[NSEW]|[NSEW]\\d{1,2}\\.\\d{1,2}°"

colon_pattern <- "[NSEW]?\\d{1,2}:\\d{1,2}:\\d{1,2}?"

normal_pattern <- "\\d{1,2}.*◦\\s*\\d{1,2}.*[NSEW]|\\d{1,2}.*°\\s*\\d{1,2}.*[NSEW]"

alt_pattern <- "\\d{1,2}.*\\s*\\d{1,2}.*[NSEW]"

df <- tibble(df)

x <- df |>
  mutate(dec = str_extract_all(text, decimal_pattern), 
         colon = str_extract_all(text, colon_pattern), 
         normal = str_extract_all(text, normal_pattern)) |>
  select(doc_id, dec, colon, normal)

df2 <- df_2 |>
  hoist("degrees") |>
  hoist("decimal_coords") |>
  hoist("colon_coords") |>
  pivot_longer(names_to = "pattern", values_to = "coords", 5:8) |>
  unnest("coords") 


df2

|>
  mutate(degree = ifelse(pattern == "normal", str_extract_all(coords, "\\d{1,2}?°|\\d{1,2}?◦"),
                         ifelse(pattern == "colon", str_extract(coords, "\\d{1,2}:"), 
                                str_extract_all(coords, "\\d{1,2}\\.\\d{1,2}"))),
         minutes = ifelse(pattern == "normal", str_extract_all(coords, "°\\d{1,2}|◦\\d{1,2}"),
                         ifelse(pattern == "colon", str_extract(coords, ":\\d{1,2}"), coords)),
) |>
  unnest("degree") |>
  unnest("minutes") |>
  mutate(degree = parse_number(degree),
         minutes = parse_number(minutes),
         decimal = ifelse(pattern != "dec", degree + (minutes/60), degree), 
         decimal = round(decimal, 4), 
         point = str_extract_all(coords, "[NSEW]")) |>
  unnest("point") |>
  mutate(lat_long = case_when(decimal < 10 & point %in% c("E", "W") ~"long", 
                              decimal > 10 & point %in% c("N", "S") ~ "lat", 
                              )) |>
  drop_na() |>
  mutate(
         decimal = ifelse(point == "W", -decimal, decimal)
         ) |>
  distinct() 


```

```{r}

df2 <- df1 |>
  select(doc_id, coords, decimal, lat_long) |>
  pivot_wider(names_from = "lat_long", values_from = "decimal") |>
  unnest("lat") |>
  unnest("long")

df2
```

We need to filter...not sure how to do this programmatically

Lets create a map...we'll use the simple features package (`sf`).

```{r}

library(sf);library(ggspatial)

df_3 <- st_as_sf(df2, coords = c(x = "long",  y ="lat"), crs = 4326)

df_3 |>
  mapview::mapview(popup = popupTable(df_3, zcol = c("doc_id"))) 
 
```

\tex
